\section{Introduction}

Convolutional Neural Network (CNN) has become a state-of-the-art algorithm for a wide range of applications like image classification~\cite{simonyan2014very}\cite{he2015deep} , object detection~\cite{redmon2015you} and other image based tasks. Compared with traditional hand-crafted feature based methods, CNN introduces a uniform model for different tasks and adjusts the model based on different training data set. Thus CNN can be adopted in different tasks and keeps high classification accuracy or detection accuracy.

But CNN is still not widely applied in real applications because of its high computation and memory complexity. A typical network like AlexNet~\cite{krizhevsky2012imagenet} consists of more than 240MB parameters and 1.4GFLOPs for the inference of a single $224\times 224$ image. More advanced networks~\cite{simonyan2014very}\cite{he2015deep} requires much more computation and memory than AlexNet. The energy cost for CNN computation is thus high, especially on traditional platforms like CPU.

Various works explore energy efficient hardware designs for CNN accelerators. One kind of researches base on CMOS technology and focus on efficient data path and memory system designs, for example the data tiling strategy in~\cite{zhang2015optimizing} and the convolution kernel in~\cite{qiu2016going}\cite{du2015shidiannao}. In these designs, the on-chip memory is implemented with SRAM, which means the size is quite limited. Thus external memory like DRAM is always needed in real applications, which means a large energy cost on the off-chip data transfer. This energy cost greatly limits the energy efficiency of this kind of design.

One solution to reduce memory access is to perform in memory computing. RRAM cross bar based design is one of the popular research topics. Chi, et al. proposes the PRIME~\cite{chi2016prime} architecture which implements the matrix vector multiplication directly with the RRAM cross bar. Work by Cheng et al.~\cite{cheng2017time} implements the RRAM cross bar to support not only the inference but also the training phase of neural networks. In memory computation with RRAM shows great energy efficiency. But the application range of the design is limited by the scalability and computation accuracy of the RRAM cross bar.

Another way to reduce off-chip data transfer is to implement large on-chip memory, where RRAM is a good candidate. We compare RRAM with SRAM and DRAM in Table~\ref{tab:ram}.

\input{table/ram.tex}

Compared with SRAM, the storage density of RRAM is much higher. A single SRAM bit can occupies more than $100\sim 200F^2$ area~\cite{ee598} while that of RRAM can be down to $6F^2$~\cite{fackenthal201419}, which is similar to DRAM. Compared with DRAM, RRAM can be integrated on-chip while the former one can not. These indicate that RRAM is a good candidate for large on-chip memory.

But RRAM is also limited in some aspects. Compared with SRAM, RRAM needs much more read/write dynamic energy and has much smaller bandwidth and higher latency, especially on the write port. For applications like CPU cache, where latency is critical to performance, RRAM may not be a good choice. For CNN computation, we think these limitations can be relieved for the following reasons:
\begin{itemize}
\item {The data access pattern can be organized beforehand. This means we care more on throughput rather than latency. More banks can be used to compensate for the bandwidth.}
\item {The chance of data reuse in CNN computation means there will be much more memory read than memory write.}
\end{itemize}

In this paper, we introduce RRAM as on-chip memory for a typical CNN accelerator design to reduce the system energy cost. The contributions of this paper is as follows:
\begin{itemize}
\item {Dedicated scheduling strategy optimization is proposed to fully utilize the RRAM buffer.}
\item {Hardware optimization is proposed to reduce the RRAM dynamic energy overhead.}
\item {Design space exploration is done with state-of-the-art networks to show the effect of using RRAM as on-chip buffer.}
\end{itemize}
Experimental results show that DDR access energy is reduced by $25\%\sim 98\%$ and on-chip buffer dynamic energy is reduced by $86\%$ on a state-of-the-art CNN model with the proposed methods. Introducing RRAM as on-chip buffer, the total energy cost of the system is reduced by $18\%$ compared with the optimal SRAM design.

The rest of this paper is organized as follows: Section II introduces the related work for CNN accelerator and RRAM research. Section III introduces the hardware design. The schedule methods are introduced in section IV. The experimental results are shown in section V. Section VI concludes this paper.